{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938ff2b-1f44-444a-be05-78e0abd50457",
   "metadata": {},
   "outputs": [],
   "source": [
    "##A2\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "# Applying RandomSearchCV() for a single perceptron AND gate\n",
    "print(\" finding AND gate using the suitable Hyperparameters and applying RandomSearchCV()\")\n",
    "# Generate a random classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "# Define the model and hyperparameter distributions\n",
    "model = LogisticRegression(random_state=42)\n",
    "param_dist = {'C': uniform(loc=0, scale=4),\n",
    "              'penalty': ['l1', 'l2']}\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
    "                                   n_iter=100, cv=5, random_state=42,\n",
    "                                   n_jobs=-1, verbose=1)\n",
    "# Fit the RandomizedSearchCV object to the data\n",
    "random_search.fit(X, y)\n",
    "# Print the best hyperparameters and score\n",
    "print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
    "print(f\"Best score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec31e3-a211-426e-aa98-0d8fbe504c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##A3\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import uniform, randint\n",
    "# Generate a random classification dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "#storing all the necessary classifiers that are required\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state=42),\n",
    "    SVC(probability=True),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    CatBoostClassifier(random_state=42, verbose=0),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False),\n",
    "    GaussianNB()\n",
    "]\n",
    "#setting the parameter descreptions \n",
    "param_dists = [\n",
    "    {'C': uniform(loc=0, scale=4), 'penalty': ['l2']},  # Only 'l2' penalty for Logistic Regression\n",
    "    {'C': uniform(loc=0, scale=4), 'gamma': uniform(loc=0, scale=1), 'kernel': ['rbf', 'poly', 'sigmoid']},\n",
    "    {'max_depth': randint(low=1, high=20), 'min_samples_leaf': randint(low=1, high=10), 'min_samples_split': randint(low=2, high=10)},\n",
    "    {'max_depth': randint(low=1, high=20), 'n_estimators': randint(low=50, high=200), 'min_samples_split': randint(low=2, high=10)},\n",
    "    {'depth': randint(low=4, high=10), 'iterations': randint(low=50, high=200), 'learning_rate': uniform(loc=0.01, scale=0.5)},\n",
    "    {'n_estimators': randint(low=50, high=200), 'learning_rate': uniform(loc=0.1, scale=5)},\n",
    "    {'max_depth': randint(low=3, high=10), 'n_estimators': randint(low=50, high=200), 'learning_rate': uniform(loc=0.01, scale=0.5)},\n",
    "    {'var_smoothing': uniform(loc=1e-11, scale=1e-7)}\n",
    "]\n",
    "print(\"{:<30} {:<50} {:<15}\".format('Classifier', 'Best Hyperparameters', 'Best Score'))\n",
    "print(\"=\"*100)\n",
    "for model, param_dist in zip(classifiers, param_dists):\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=25, cv=5, random_state=42, n_jobs=-1, verbose=0)\n",
    "    random_search.fit(X, y)\n",
    "    print(\"{:<30} {:<50} {:<15}\".format(model._class.name, str(random_search.best_params), \"{:.4f}\".format(random_search.best_score_)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
